# -*- coding: utf-8 -*-
"""DL_project_backend.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E_bgKGQJmf4n8c0XdkFI6mBHaZ6379Ur
"""

!pip install diffusers transformers accelerate torch torchvision

!pip install gradio

!pip install flask peft huggingface_hub

!pip install pyngrok

# Install required libraries
from flask import Flask, request, jsonify
from diffusers import StableDiffusionPipeline
from pyngrok import ngrok
import base64
import io
import os
import torch
from diffusers import DiffusionPipeline, AutoencoderKL
from diffusers import StableDiffusionXLPipeline

import torch
print("Is GPU available:", torch.cuda.is_available())

# Set up project structure
PROJECT_DIR = "text_to_image_project"
OUTPUT_DIR = os.path.join(PROJECT_DIR, "outputs")
MODEL_DIR = os.path.join(PROJECT_DIR, "models")
LOG_FILE = os.path.join(PROJECT_DIR, "feedback_log.csv")

# Function to create directories
def create_project_structure():
    os.makedirs(PROJECT_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(MODEL_DIR, exist_ok=True)
    if not os.path.exists(LOG_FILE):
        with open(LOG_FILE, "w") as f:
            f.write("prompt,rating\n")  # Create a feedback log with headers

# Check GPU availability
def check_gpu():
    gpu_available = torch.cuda.is_available()
    print(f"Is GPU available: {gpu_available}")
    if not gpu_available:
        print("GPU not available. Switch to Google Colab or Kaggle with GPU runtime.")
        exit()

def generate_sample_image(pipe, prompt=):
    print(f"Generating image for prompt: {prompt}")
    image = pipe(prompt).images[0]  # Generate image
    image.show()  # Display the image
    output_path = os.path.join(OUTPUT_DIR, "test_image.png")
    image.save(output_path)  # Save the image
    print(f"Image saved to {output_path}")

from pyngrok import ngrok

# Replace <YOUR_AUTHTOKEN> with the token you copied from ngrok
ngrok.set_auth_token("2qHIXBs799JYdxrdkFTAmvFuPkv_5WX6YVaoReGpiCwSFTeza")

import torch
from diffusers import DiffusionPipeline, AutoencoderKL

vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
pipe = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    vae=vae,
    torch_dtype=torch.float16,
    variant="fp16",
    use_safetensors=True
)
pipe.load_lora_weights("nikhilsoni700/dl_project_LoRA")
_ = pipe.to("cuda" if torch.cuda.is_available() else "cpu")

from flask import Flask, request, jsonify
from pyngrok import ngrok

app = Flask(__name__)

@app.route("/generate", methods=["POST"])
def generate_image():
    data = request.get_json()
    prompt = data.get("prompt", "")
    guidance_scale = data.get("guidance_scale", 7.5)
    num_inference_steps = data.get("num_inference_steps", 50)

    image = pipe(prompt, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]

    # Convert image to base64
    import io, base64
    buffer = io.BytesIO()
    image.save(buffer, format="PNG")
    buffer.seek(0)
    image_b64 = base64.b64encode(buffer.getvalue()).decode()

    return jsonify({"image": image_b64})

# Start the ngrok tunnel
public_url = ngrok.connect(5000)
print(f"Public URL: {public_url}")

app.run(port=5000)

